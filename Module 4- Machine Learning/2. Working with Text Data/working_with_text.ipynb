{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text Data - Text Preprocessing and Feature Extraction (Text to Numerical Vector)\n",
    "\n",
    "##### Text Data\n",
    "\n",
    "Text Analysis is a major application field for machine learning algorithms. Some of the major application areas of NLP are:\n",
    "\n",
    "1. Spell Checker, Keyword Search, etc\n",
    "2. Sentiment Analysis, Spam Classification\n",
    "3. Machine Translation\n",
    "4. Chatbots/Dialog Systems\n",
    "5. Question Answering Systems\n",
    "etc..\n",
    "\n",
    "However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "##### Why NLP is hard?\n",
    "1. Complexity of representation\n",
    " Poems, Sarcasm, etc...\n",
    " Example 1: This task is a piece of cake.\n",
    " Example 2: You have a football game tomorrow. Break a leg!\n",
    "\n",
    "2. Ambiguity in Natural Language\n",
    " Ambiguity means uncertainity of meaning.\n",
    " For Example: The car hit the pole while it was moving.\n",
    "\n",
    "#### Text Preprocessing\n",
    "\n",
    "1. Removing special characters and punctuations\n",
    "2. Convert sentence into lower case\n",
    "3. Tokenisation\n",
    "4. Removing stop words\n",
    "5. Stemming or Lemmatization\n",
    "\n",
    "### Feature Extraction Techniques (Convert Text to Numerical Vectors)\n",
    "\n",
    "1. Bag of Words\n",
    "2. TF IDF (Term Frequency - Inverse Document Frequency)\n",
    "3. Word2Vec (by Google)\n",
    "4. GloVe (Global Vectors by Stanford) - Not Covered in this notebook\n",
    "5. Pretrained GloVe Embeddings\n",
    "6. FastText (by Facebook) - Not Covered in this notebook\n",
    "7. ELMo (Embeddings from Language Models) - Not Covered in this notebook\n",
    "8. BERT (Bidirectional Encoder Representations from Transformer)\n",
    "9. GPT\n",
    "10. LLM's\n",
    "\n",
    "### Text Preprocessing Steps\n",
    "\n",
    "Text Preprocessing steps include some essential tasks to clean and remove the noise from the available data.\n",
    "\n",
    "1. Removing Special Characters and Punctuation - Special characters like ^, ~, @, $, etc... Punctuations like ., ?, ,, etc...\n",
    "\n",
    "2. Converting to Lower Case - We convert the whole text corpus to lower case to reduce the size of the vocabulary of our text data.\n",
    "\n",
    "3. Tokenization (Sentence Tokenization and Word Tokenization) - This is a simple step to break the text into sentences or words.\n",
    "\n",
    "4. Removing Stop Words - Stopwords don't contribute to the meaning of a sentence. So, we can safely remove them without changing the meaning of the sentence. For eg: it, was, any, then, a, is, by, etc are the stopwords.\n",
    "\n",
    "5. Stemming or Lemmatization - Stemming is the process of removing suffixes and reducing a word to some root form. For eg: warm, warmer, warming can be converted to warm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install nltk\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/idreesy31/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/idreesy31/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/idreesy31/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/idreesy31/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the punctions\n",
    "nltk.download('punkt')\n",
    "# Download the stopwords\n",
    "nltk.download('stopwords')\n",
    "# Downloading wordnet before applying lemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 1is Natural-LAnguage-Processing. In this example wE are goIng to Learn variouS text9 preprocessing steps.\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"This 1is Natural-LAnguage-Processing. In this example wE are goIng to Learn variouS text9 preprocessing steps.\"\n",
    "\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This  is Natural LAnguage Processing. In this example wE are goIng to Learn variouS text  preprocessing steps.\n"
     ]
    }
   ],
   "source": [
    "# Removing special characters and digits\n",
    "text = re.sub(\"[^a-zA-z.]\", ' ', raw_text )\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this  is natural language processing. in this example we are going to learn various text  preprocessing steps.\n"
     ]
    }
   ],
   "source": [
    "# change sentence to lower case\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this  is natural language processing.', 'in this example we are going to learn various text  preprocessing steps.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize text into sentences\n",
    "my_sentences = sent_tokenize(text)\n",
    "print(my_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'natural', 'language', 'processing', '.']\n",
      "['in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences to words\n",
    "for sentence in my_sentences:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'natural', 'language', 'processing', '.', 'in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text to words\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '.', 'example', 'going', 'learn', 'various', 'text', 'preprocessing', 'steps', '.']\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words\n",
    "words = [word for word in words if word not in stopwords.words('english')]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'is', 'natur', 'languag', 'process', '.', 'in', 'thi', 'exampl', 'we', 'are', 'go', 'to', 'learn', 'variou', 'text', 'preprocess', 'step', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "## initialize the inbuilt stemmer\n",
    "stemmer = PorterStemmer()\n",
    "clean_tokens_stem =[stemmer.stem(word) for word in words]\n",
    "print(clean_tokens_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'natural', 'language', 'processing', '.', 'in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'step', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing\n",
    "## We can also use lemmatizer instead of Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_tokens_lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(clean_tokens_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text\n",
       "0     it Was the best oF Times $\n",
       "1     It was The worst of times.\n",
       "2     IT 9 was tHe age Of wisdom\n",
       "3  it was thE age of foolishness"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_text = ['it Was the best oF Times $', \n",
    "            'It was The worst of times.',\n",
    "            'IT 9 was tHe age Of wisdom', \n",
    "            'it was thE age of foolishness']\n",
    "\n",
    "df = pd.DataFrame({'text': lst_text})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text, flag):\n",
    "    # Removing special characters and digits\n",
    "    text = re.sub(\"[^a-zA-Z]\", ' ', raw_text)\n",
    "\n",
    "    # change text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "    # stemming/lemmatization\n",
    "    if(flag == 'stem'):\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    preprocessed_text = \" \".join(words)\n",
    "    words_in_preprocessed_text = len(words)\n",
    "    return pd.Series([preprocessed_text, words_in_preprocessed_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1\n",
       "0    best time  2\n",
       "1   worst time  2\n",
       "2   age wisdom  2\n",
       "3  age foolish  2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df['text'].apply(lambda x: preprocess(x, 'stem'))\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_text_stem  text_length_stem\n",
       "0       best time                 2\n",
       "1      worst time                 2\n",
       "2      age wisdom                 2\n",
       "3     age foolish                 2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.columns = ['clean_text_stem', 'text_length_stem']\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem\n",
       "0     it Was the best oF Times $       best time                 2\n",
       "1     It was The worst of times.      worst time                 2\n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2\n",
       "3  it was thE age of foolishness     age foolish                 2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df,temp_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  1\n",
       "0        best time  2\n",
       "1       worst time  2\n",
       "2       age wisdom  2\n",
       "3  age foolishness  2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df['text'].apply(lambda x: preprocess(x, 'lemma'))\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_text_lemma  text_length_lemma\n",
       "0        best time                  2\n",
       "1       worst time                  2\n",
       "2       age wisdom                  2\n",
       "3      age foolish                  2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.columns = ['clean_text_lemma', 'text_length_lemma']\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma  \n",
       "0        best time                  2  \n",
       "1       worst time                  2  \n",
       "2       age wisdom                  2  \n",
       "3      age foolish                  2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df,temp_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word Representation\n",
    "\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "We will use CountVectorizer to convert text into a matrix of token count.\n",
    "\n",
    "Bag of Words: https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "\n",
    "Code Example: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "\n",
    "##### We are going to perform below mentioned steps to understand the entire process:\n",
    "a. Converting text to numerical vectors with the help of CountVectorizer\n",
    "b. Understand fit and transform\n",
    "c. Looking at vocabulary_\n",
    "d. Converting sparse matrix to dense matrix using toarray()\n",
    "e. Understanding n_gram\n",
    "\n",
    "Advantages\n",
    "1. It is simple to understand and implement like OneHotEncoding.\n",
    "2. We have a fixed length encoding for any sequence of arbitrary length.\n",
    "3. Documents with same words/vocabulary will have similar representation. So if two documents have a similar vocabulary, they’ll be closer to each other in the vector space and vice versa.\n",
    "\n",
    "Disadvantages\n",
    "1. The size of vector increases with the size of the vocabulary. Thus, sparsity continues to be a problem. One way to control it is by limiting the vocabulary to n number of the most frequent words.\n",
    "2. It does not capture the similarity between different words that mean the same thing. i.e. Semantic Meaning is not captured.\n",
    " a. \"walk\", \"walked\", and \"walking\". BoW vectors of all three tokens will be equally apart.\n",
    " b. \"search\" and \"explore\" are synonyms. BoW won't capture the semantic similarity of these words.\n",
    "\n",
    "3. This representation does not have any way to handle out of vocabulary (OOV) words (i.e., new words that were not seen in the corpus that was used to build the vectorizer).\n",
    "4. As the name indicates, it is a “bag” of words. Word order information is lost in this representation. One way to control it is by using n-grams.\n",
    "5. It suffers from curse of high dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma  \n",
       "0        best time                  2  \n",
       "1       worst time                  2  \n",
       "2       age wisdom                  2  \n",
       "3      age foolish                  2  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "vocab = CountVectorizer()\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "\n",
    "dtm = vocab.fit_transform(df['clean_text_lemma'])\n",
    "# fit_transform() could be done seperatly as mentioned below\n",
    "# vocab.fit(df.clean_text_stem)\n",
    "# dtm = vocab.transform(df.clean_text_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best': 1, 'time': 3, 'worst': 5, 'age': 0, 'wisdom': 4, 'foolish': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can look at unique words by using 'vocabulary_'\n",
    "vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Observe that the type of dtm is sparse\n",
    "\n",
    "print(type(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n"
     ]
    }
   ],
   "source": [
    "# Lets now print the  shape of this dtm\n",
    "\n",
    "print(dtm.shape)\n",
    "\n",
    "# o/p -> (4, 6)\n",
    "# i.e -> 4 documents and 6 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the dtm\n",
    "\n",
    "print(dtm)\n",
    "\n",
    "# Remember that dtm is a sparse matrix. i.e. zeros wont be stored\n",
    "# Lets understand First line of output -> (0,1)    1\n",
    "# Here (0, 1) means 0th document and 1st(index starting from 0) unique word. \n",
    "# (we have total 4 documents) & (we have total 6 unique words)\n",
    "# (0, 1)    1 -> 1 here refers to the number of occurence of 1st word\n",
    "# Now lets read it all in english.\n",
    "# (0, 1)    1 -> 'times' occurs 1 time in 0th document. \n",
    "# Try to observe -> (3, 2)   1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0]\n",
      " [0 0 0 1 0 1]\n",
      " [1 0 0 0 1 0]\n",
      " [1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Since the dtm is sparse, lets convert it into numpy array.\n",
    "\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'best', 'foolish', 'time', 'wisdom', 'worst']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolish</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  best  foolish  time  wisdom  worst\n",
       "0    0     1        0     1       0      0\n",
       "1    0     0        0     1       0      1\n",
       "2    1     0        0     0       1      0\n",
       "3    1     0        1     0       0      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vocab.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-grams\n",
    "vocab = CountVectorizer(ngram_range=(1,2))\n",
    "dtm = vocab.fit_transform(df.clean_text_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best': 3, 'time': 6, 'best time': 4, 'worst': 8, 'worst time': 9, 'age': 0, 'wisdom': 7, 'age wisdom': 2, 'foolish': 5, 'age foolish': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 2)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 1]\n",
      " [1 0 1 0 0 0 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# convert sparse matrix to numpy array\n",
    "\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age foolish</th>\n",
       "      <th>age wisdom</th>\n",
       "      <th>best</th>\n",
       "      <th>best time</th>\n",
       "      <th>foolish</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "      <th>worst time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  age foolish  age wisdom  best  best time  foolish  time  wisdom  \\\n",
       "0    0            0           0     1          1        0     1       0   \n",
       "1    0            0           0     0          0        0     1       0   \n",
       "2    1            0           1     0          0        0     0       1   \n",
       "3    1            1           0     0          0        1     0       0   \n",
       "\n",
       "   worst  worst time  \n",
       "0      0           0  \n",
       "1      1           1  \n",
       "2      0           0  \n",
       "3      0           0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vocab.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "vect.fit(lst_text) learns the vocabulary\n",
    "vect.transform(lst_text) uses the fitted vocabulary to build a document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency Inverse Document Frequency\n",
    "\n",
    "In BOW approach all the words in the text are treated as equally important i.e. there's no notion of some words in the document being more important than others. TF-IDF, or term frequency-inverse document frequency, addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus.\n",
    "\n",
    "Let's now try to understand:\n",
    "\n",
    "Term Frequency\n",
    "Inverse Document Frequency\n",
    " \n",
    " \n",
    "Advantages\n",
    "If the word is rare in the corpus, it will be given more importance. (i.e. IDF)\n",
    "If the word is more frequent in a document, it will be given more importance. (i.e. TF)\n",
    "\n",
    "Disadvantages\n",
    " Same as BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "vectorizer = TfidfVectorizer()\n",
    "dtm = vectorizer.fit_transform(df.clean_text_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best': 1, 'time': 3, 'worst': 5, 'age': 0, 'wisdom': 4, 'foolish': 2}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.78528828 0.         0.6191303  0.         0.        ]\n",
      " [0.         0.         0.         0.6191303  0.         0.78528828]\n",
      " [0.6191303  0.         0.         0.         0.78528828 0.        ]\n",
      " [0.6191303  0.         0.78528828 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(dtm.toarray())\n",
    "\n",
    "# convert sparse matrix to nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolish</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age      best   foolish     time    wisdom     worst\n",
       "0  0.00000  0.785288  0.000000  0.61913  0.000000  0.000000\n",
       "1  0.00000  0.000000  0.000000  0.61913  0.000000  0.785288\n",
       "2  0.61913  0.000000  0.000000  0.00000  0.785288  0.000000\n",
       "3  0.61913  0.000000  0.785288  0.00000  0.000000  0.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space\n",
    "\n",
    "A latent space, also known as a latent feature space or embedding space, is an embedding of a set of items within a manifold in which items which resemble each other more closely are positioned closer to one another in the latent space.\n",
    "\n",
    "### Word Embeddings (Word Vectors)\n",
    "\n",
    "In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.\n",
    "\n",
    "Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.\n",
    "\n",
    "Traditionally, one of the main limitations of word embeddings (word vector space models in general) is that words with multiple meanings are conflated into a single representation (a single vector in the semantic space). In other words, polysemy and homonymy are not handled properly.\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "\"You shall know the word by the company it keeps.\" by JR Firth\n",
    "\n",
    "Distributional Semantics (i.e. a word is characterized by the company it keeps)\n",
    "W2v works well because there is an idea of meaning distribution in the context.\n",
    "\n",
    "Algorithms to generate Word2Vec Embeddings\n",
    "\n",
    "1. SkipGram\n",
    "2. Continuous Bag of Words\n",
    "\n",
    "Issue\n",
    "Even if the word is having three different meaning, W2v will return the weighted average of all three as the output. Now the question is,\n",
    "\n",
    "Is it possible to segregate the three vectors to represent the words based in the context?\n",
    "$$ OR $$\n",
    "Is it possible to disambiguate the word vectors based on the context?\n",
    "Word2Vec is not capturing the contextual information. This is where BERT comes handy.\n",
    "\n",
    "! pip install gensim\n",
    "! pip install --upgrade gensim\n",
    "Run this in command promp (admin mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.6 MB 1.4 kB/s eta 0:00:01    |▏                               | 163 kB 101 kB/s eta 0:04:21     |█                               | 768 kB 427 kB/s eta 0:01:01     |█▉                              | 1.5 MB 395 kB/s eta 0:01:04     |██▍                             | 2.0 MB 589 kB/s eta 0:00:42     |██▊                             | 2.3 MB 544 kB/s eta 0:00:45     |███▏                            | 2.6 MB 544 kB/s eta 0:00:44     |███▉                            | 3.2 MB 937 kB/s eta 0:00:25     |█████▎                          | 4.3 MB 527 kB/s eta 0:00:43     |██████▋                         | 5.5 MB 1.9 MB/s eta 0:00:12     |██████▊                         | 5.6 MB 1.9 MB/s eta 0:00:12     |██████▉                         | 5.7 MB 1.9 MB/s eta 0:00:12     |███████                         | 5.8 MB 1.9 MB/s eta 0:00:12     |███████▏                        | 5.9 MB 1.9 MB/s eta 0:00:12     |███████▋                        | 6.3 MB 529 kB/s eta 0:00:39     |███████▊                        | 6.4 MB 529 kB/s eta 0:00:39     |████████                        | 6.6 MB 529 kB/s eta 0:00:38     |████████▌                       | 7.1 MB 686 kB/s eta 0:00:29     |█████████▌                      | 7.9 MB 1.1 MB/s eta 0:00:17     |█████████▊                      | 8.1 MB 290 kB/s eta 0:01:04     |██████████▎                     | 8.5 MB 290 kB/s eta 0:01:03     |██████████▎                     | 8.6 MB 290 kB/s eta 0:01:03     |██████████▍                     | 8.6 MB 290 kB/s eta 0:01:02     |██████████▋                     | 8.8 MB 290 kB/s eta 0:01:02     |███████████▎                    | 9.3 MB 1.3 MB/s eta 0:00:13     |███████████▊                    | 9.7 MB 1.3 MB/s eta 0:00:13     |████████████                    | 10.0 MB 1.1 MB/s eta 0:00:15     |████████████▏                   | 10.1 MB 1.1 MB/s eta 0:00:15     |████████████▍                   | 10.3 MB 1.1 MB/s eta 0:00:15     |████████████▉                   | 10.6 MB 337 kB/s eta 0:00:48     |█████████████▎                  | 11.0 MB 337 kB/s eta 0:00:47     |█████████████▋                  | 11.3 MB 337 kB/s eta 0:00:46     |█████████████▊                  | 11.4 MB 337 kB/s eta 0:00:45     |█████████████▉                  | 11.5 MB 470 kB/s eta 0:00:32     |██████████████                  | 11.6 MB 470 kB/s eta 0:00:32     |██████████████                  | 11.7 MB 470 kB/s eta 0:00:32     |██████████████▎                 | 11.9 MB 470 kB/s eta 0:00:32     |██████████████▌                 | 12.0 MB 470 kB/s eta 0:00:31     |██████████████▋                 | 12.1 MB 470 kB/s eta 0:00:31     |██████████████▋                 | 12.2 MB 470 kB/s eta 0:00:31     |███████████████▊                | 13.0 MB 470 kB/s eta 0:00:29     |████████████████▌               | 13.7 MB 1.1 MB/s eta 0:00:12        | 14.3 MB 1.1 MB/s eta 0:00:11     |█████████████████▌              | 14.6 MB 1.1 MB/s eta 0:00:11     |█████████████████▊              | 14.7 MB 1.1 MB/s eta 0:00:11     |█████████████████▉              | 14.8 MB 1.1 MB/s eta 0:00:11     |██████████████████▍             | 15.3 MB 1.1 MB/s eta 0:00:11     |██████████████████▊             | 15.5 MB 1.1 MB/s eta 0:00:11     |██████████████████▉             | 15.6 MB 1.1 MB/s eta 0:00:11     |███████████████████             | 15.7 MB 1.1 MB/s eta 0:00:11     |███████████████████             | 15.7 MB 1.1 MB/s eta 0:00:11     |███████████████████             | 15.8 MB 1.1 MB/s eta 0:00:11     |███████████████████             | 15.9 MB 1.1 MB/s eta 0:00:11     |███████████████████▍            | 16.1 MB 744 kB/s eta 0:00:15     |███████████████████▊            | 16.3 MB 744 kB/s eta 0:00:14     |███████████████████▊            | 16.4 MB 744 kB/s eta 0:00:14     |████████████████████            | 16.7 MB 1.4 MB/s eta 0:00:08     |████████████████████▏           | 16.8 MB 1.4 MB/s eta 0:00:08     |████████████████████▉           | 17.3 MB 1.4 MB/s eta 0:00:07     |█████████████████████           | 17.5 MB 1.4 MB/s eta 0:00:07     |█████████████████████▎          | 17.6 MB 1.4 MB/s eta 0:00:07     |█████████████████████▎          | 17.7 MB 1.4 MB/s eta 0:00:07     |█████████████████████▍          | 17.7 MB 1.4 MB/s eta 0:00:07     |██████████████████████          | 18.2 MB 1.5 MB/s eta 0:00:06     |██████████████████████▏         | 18.4 MB 1.5 MB/s eta 0:00:06     |██████████████████████▎         | 18.4 MB 1.5 MB/s eta 0:00:06     |██████████████████████▎         | 18.5 MB 1.5 MB/s eta 0:00:06     |██████████████████████▍         | 18.6 MB 1.5 MB/s eta 0:00:06     |██████████████████████▌         | 18.7 MB 1.5 MB/s eta 0:00:06     |██████████████████████▌         | 18.7 MB 1.5 MB/s eta 0:00:06     |██████████████████████▋         | 18.7 MB 1.5 MB/s eta 0:00:06     |██████████████████████▋         | 18.8 MB 1.5 MB/s eta 0:00:06     |██████████████████████▊         | 18.9 MB 1.5 MB/s eta 0:00:06     |██████████████████████▉         | 19.0 MB 937 kB/s eta 0:00:09     |███████████████████████         | 19.0 MB 937 kB/s eta 0:00:09     |███████████████████████         | 19.1 MB 937 kB/s eta 0:00:08     |███████████████████████         | 19.2 MB 937 kB/s eta 0:00:08     |███████████████████████▏        | 19.2 MB 937 kB/s eta 0:00:08     |███████████████████████▏        | 19.3 MB 937 kB/s eta 0:00:08     |███████████████████████▎        | 19.3 MB 937 kB/s eta 0:00:08     |███████████████████████▌        | 19.5 MB 937 kB/s eta 0:00:08     |███████████████████████▋        | 19.6 MB 937 kB/s eta 0:00:08     |███████████████████████▋        | 19.6 MB 937 kB/s eta 0:00:08     |████████████████████████▉       | 20.6 MB 937 kB/s eta 0:00:07     |█████████████████████████▍      | 21.1 MB 2.6 MB/s eta 0:00:03     |█████████████████████████▋      | 21.2 MB 2.6 MB/s eta 0:00:03     |█████████████████████████▊      | 21.3 MB 2.6 MB/s eta 0:00:03     |██████████████████████████      | 21.5 MB 2.6 MB/s eta 0:00:02     |██████████████████████████      | 21.7 MB 2.6 MB/s eta 0:00:02     |██████████████████████████▏     | 21.7 MB 2.6 MB/s eta 0:00:02     |██████████████████████████▎     | 21.8 MB 2.6 MB/s eta 0:00:02     |██████████████████████████▌     | 22.0 MB 2.6 MB/s eta 0:00:02     |██████████████████████████▊     | 22.1 MB 2.6 MB/s eta 0:00:02     |██████████████████████████▉     | 22.3 MB 1.4 MB/s eta 0:00:04     |███████████████████████████     | 22.4 MB 1.4 MB/s eta 0:00:04     |███████████████████████████▍    | 22.8 MB 1.4 MB/s eta 0:00:03     |███████████████████████████▊    | 23.0 MB 1.4 MB/s eta 0:00:03     |███████████████████████████▉    | 23.1 MB 1.4 MB/s eta 0:00:03     |████████████████████████████▌   | 23.6 MB 1.4 MB/s eta 0:00:03     |████████████████████████████▋   | 23.7 MB 558 kB/s eta 0:00:06     |█████████████████████████████   | 24.0 MB 558 kB/s eta 0:00:05     |█████████████████████████████   | 24.1 MB 558 kB/s eta 0:00:05     |█████████████████████████████▎  | 24.3 MB 558 kB/s eta 0:00:05     |█████████████████████████████▋  | 24.6 MB 558 kB/s eta 0:00:04     |██████████████████████████████  | 24.8 MB 558 kB/s eta 0:00:04     |██████████████████████████████  | 24.9 MB 558 kB/s eta 0:00:03     |██████████████████████████████▍ | 25.3 MB 558 kB/s eta 0:00:03     |██████████████████████████████▌ | 25.3 MB 558 kB/s eta 0:00:03     |██████████████████████████████▉ | 25.6 MB 1.0 MB/s eta 0:00:01     |███████████████████████████████ | 25.6 MB 1.0 MB/s eta 0:00:01     |███████████████████████████████ | 25.7 MB 1.0 MB/s eta 0:00:01     |███████████████████████████████ | 25.7 MB 1.0 MB/s eta 0:00:01     |███████████████████████████████▏| 25.9 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 800 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /home/idreesy31/.local/lib/python3.9/site-packages (from gensim) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/idreesy31/.local/lib/python3.9/site-packages (from gensim) (1.11.4)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.2\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>[best, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>[worst, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, wisdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, foolish]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma tokenised_sentences  \n",
       "0        best time                  2        [best, time]  \n",
       "1       worst time                  2       [worst, time]  \n",
       "2       age wisdom                  2       [age, wisdom]  \n",
       "3      age foolish                  2      [age, foolish]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenised_sentences'] = df.clean_text_stem.apply(lambda sent : sent.split())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['best', 'time'], ['worst', 'time'], ['age', 'wisdom'], ['age', 'foolish']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.tokenised_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = Word2Vec(list(df.tokenised_sentences), vector_size=100, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=6, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Documents\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 0, 'time': 1, 'foolish': 2, 'wisdom': 3, 'worst': 4, 'best': 5}\n",
      "['age', 'time', 'foolish', 'wisdom', 'worst', 'best']\n"
     ]
    }
   ],
   "source": [
    "# looking at the vocabulary\n",
    "print(model.wv.key_to_index)\n",
    "print(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# access the 100 dimensional vector for one of the words\n",
    "\n",
    "print(model.wv.__getitem__('foolish'))\n",
    "print(model.wv.__getitem__('foolish').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      "  -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      "  -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      "  -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "   2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "   7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "   6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      "  -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "   9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "   8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      "  -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      "  -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "   4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      "  -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "   4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      "  -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      "  -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      "  -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      "  -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "   7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      "  -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      "  -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      "  -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "   3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      "  -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
      " [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
      "   7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
      "  -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
      "  -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
      "   6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
      "   2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
      "   6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
      "  -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
      "  -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
      "   6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
      "   7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
      "  -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
      "   1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
      "  -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
      "   9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
      "  -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
      "   3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
      "   7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
      "   5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
      "  -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      "  -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "   1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
      "   2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
      "   2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
      "   5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
      " [ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "   7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      "  -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "   9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "   5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      "  -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "   7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      "  -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      "  -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      "  -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      "  -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "   3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      "  -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      "  -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      "  -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "   4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      "  -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      "  -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "   4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "   9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "   1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "   1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "   8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "   9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "   5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      " [-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
      "   4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
      "   6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
      "   3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
      "   8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
      "   1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
      "  -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
      "   2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
      "  -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
      "   9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
      "   3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
      "   7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
      "  -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
      "  -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
      "   1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
      "   2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
      "   4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
      "  -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
      "  -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
      "  -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
      "  -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
      "  -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
      "  -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
      "  -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
      "   5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n",
      " [-7.1390150e-03  1.2410306e-03 -7.1767163e-03 -2.2446180e-03\n",
      "   3.7193035e-03  5.8331238e-03  1.1981833e-03  2.1027315e-03\n",
      "  -4.1103913e-03  7.2253332e-03 -6.3070417e-03  4.6472158e-03\n",
      "  -8.2199732e-03  2.0364679e-03 -4.9770521e-03 -4.2476882e-03\n",
      "  -3.1089843e-03  5.6552086e-03  5.7984008e-03 -4.9746488e-03\n",
      "   7.7333092e-04 -8.4957778e-03  7.8098057e-03  9.2572914e-03\n",
      "  -2.7423275e-03  8.0022332e-04  7.4665190e-04  5.4778848e-03\n",
      "  -8.6060790e-03  5.8445573e-04  6.8694223e-03  2.2315944e-03\n",
      "   1.1246764e-03 -9.3221553e-03  8.4823668e-03 -6.2641273e-03\n",
      "  -2.9923737e-03  3.4937870e-03 -7.7262759e-04  1.4112913e-03\n",
      "   1.7819917e-03 -6.8288995e-03 -9.7248117e-03  9.0405848e-03\n",
      "   6.1980546e-03 -6.9129276e-03  3.4034825e-03  2.0606398e-04\n",
      "   4.7537456e-03 -7.1199429e-03  4.0269541e-03  4.3474343e-03\n",
      "   9.9573694e-03 -4.4737398e-03 -1.3892639e-03 -7.3173214e-03\n",
      "  -9.6978294e-03 -9.0802573e-03 -1.0227549e-03 -6.5032900e-03\n",
      "   4.8497282e-03 -6.1640264e-03  2.5191857e-03  7.3944090e-04\n",
      "  -3.3921539e-03 -9.7922329e-04  9.9791251e-03  9.1458866e-03\n",
      "  -4.4618296e-03  9.0830270e-03 -5.6417631e-03  5.9309220e-03\n",
      "  -3.0972182e-03  3.4317516e-03  3.0172265e-03  6.9004609e-03\n",
      "  -2.3738837e-03  8.7750368e-03  7.5894282e-03 -9.5476462e-03\n",
      "  -8.0082091e-03 -7.6378966e-03  2.9232574e-03 -2.7947223e-03\n",
      "  -6.9295205e-03 -8.1282640e-03  8.3091799e-03  1.9904887e-03\n",
      "  -9.3280170e-03 -4.7927164e-03  3.1367384e-03 -4.7132061e-03\n",
      "   5.2808430e-03 -4.2334413e-03  2.6417959e-03 -8.0456873e-03\n",
      "   6.2098862e-03  4.8188888e-03  7.8719261e-04  3.0134476e-03]\n",
      " [-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
      "  -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      "  -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
      "  -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
      "  -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
      "  -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      "  -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
      "   1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
      "  -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
      "   5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
      "   9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
      "  -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
      "  -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
      "   7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
      "   9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
      "   8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "   3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
      "  -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
      "   7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
      "  -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
      "   7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
      "  -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
      "  -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
      "  -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
      "  -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Access the 100D vectors for all 6 words\n",
    "\n",
    "print(model.wv.__getitem__(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 100)\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.__getitem__(model.wv.index_to_key).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# model.save('model/first_word_vectors.bin')\n",
    "\n",
    "# # load model\n",
    "# new_model = Word2Vec.load('model/first_word_vectors.bin')\n",
    "# print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = model.wv.__getitem__(model.wv.index_to_key)\n",
    "pca = PCA(n_components = 2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.index_to_key)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13887982"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('best', 'worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 0.17018885910511017),\n",
       " ('best', 0.06408978998661041),\n",
       " ('wisdom', -0.013514933176338673),\n",
       " ('time', -0.023671666160225868),\n",
       " ('age', -0.05234673619270325)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('foolish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>[best, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>[worst, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, wisdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, foolish]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma tokenised_sentences  \n",
       "0        best time                  2        [best, time]  \n",
       "1       worst time                  2       [worst, time]  \n",
       "2       age wisdom                  2       [age, wisdom]  \n",
       "3      age foolish                  2      [age, foolish]  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Embedding (Document Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best', 'time']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove out-of-vocabulary words\n",
    "\n",
    "sentence = ['best', 'dristol', 'time', 'alade']\n",
    "\n",
    "vocab_tokens = [word for word in sentence if word in model.wv.index_to_key]\n",
    "\n",
    "vocab_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.6735850e-03,  2.8979499e-03,  2.1581696e-03, -1.7885750e-03,\n",
       "       -9.8061212e-04, -3.7891967e-03,  2.7690111e-03,  4.8756767e-03,\n",
       "       -4.6693720e-03, -6.5232953e-03, -2.7048176e-03, -5.3278962e-03,\n",
       "       -6.4251497e-03, -1.2493895e-03,  3.0445517e-04, -5.6858570e-04,\n",
       "        3.8068579e-04,  9.2990650e-04, -3.0666459e-03, -1.1344015e-03,\n",
       "       -3.3043111e-03, -2.6271159e-03,  8.2706194e-03, -1.0838672e-03,\n",
       "       -2.2073742e-04, -3.7620717e-04, -9.0713974e-04, -2.5862674e-03,\n",
       "       -1.3156777e-04,  6.6179251e-03,  7.8556351e-03, -6.5627526e-03,\n",
       "       -2.5582409e-03, -6.9178990e-03,  1.9483893e-03,  6.0251304e-03,\n",
       "        6.4321910e-03,  5.5842018e-03,  7.2997799e-03,  3.0152500e-03,\n",
       "        8.7251253e-03, -7.1729645e-03, -8.2131261e-03, -1.3105709e-03,\n",
       "       -1.9392008e-03,  2.3391065e-03,  2.6729943e-03,  2.9715800e-03,\n",
       "        4.0672242e-04,  8.2550046e-05,  5.2809850e-03, -8.9346431e-03,\n",
       "        3.8251362e-03,  6.0026506e-03, -5.2615297e-03,  5.4140193e-03,\n",
       "        9.4578769e-03, -5.6464854e-04, -3.8392697e-03, -1.3056444e-05,\n",
       "       -6.9205649e-05,  1.2628853e-03,  2.2532057e-03, -6.6235880e-03,\n",
       "        3.6663515e-03,  4.0026912e-03,  4.2561139e-03, -1.7194152e-03,\n",
       "       -1.7866492e-05, -1.3664386e-03, -6.7761191e-04, -1.2985931e-03,\n",
       "        6.7115780e-03,  6.1228252e-03, -7.8244333e-04,  7.9815416e-04,\n",
       "       -8.2111200e-03,  4.5453617e-04,  2.7050036e-03,  9.6003409e-05,\n",
       "       -9.9213817e-04, -1.4492136e-03,  5.2344799e-03, -7.9753939e-03,\n",
       "        2.0204962e-04, -5.3818068e-03, -2.5371348e-03, -2.9714126e-04,\n",
       "        4.1687128e-04, -9.1784005e-04,  5.0794631e-03,  3.6809542e-03,\n",
       "       -3.0058809e-03, -5.4469449e-03,  3.5847109e-03,  3.8838149e-03,\n",
       "        1.8792974e-03, -5.5417679e-03, -3.2742890e-03, -1.0850094e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create document vectors by averaging word vectors\n",
    "np.mean(model.wv.__getitem__(vocab_tokens), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(model.wv.__getitem__(vocab_tokens), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc, keyed_vectors):\n",
    "    \"\"\"Remove out-of-vocabulary words. Create document vectors by averaging word vectors.\"\"\"\n",
    "    vocab_tokens = [word for word in doc if word in keyed_vectors.index_to_key]\n",
    "    return np.mean(keyed_vectors.__getitem__(vocab_tokens), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "      <th>doc_vector_w2v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>[best, time]</td>\n",
       "      <td>[-0.008673585, 0.00289795, 0.0021581696, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>[worst, time]</td>\n",
       "      <td>[-0.007879351, 0.0024533845, -0.0009934164, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, wisdom]</td>\n",
       "      <td>[-0.0043894527, 0.004767893, 0.0024528443, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, foolish]</td>\n",
       "      <td>[-0.00022083164, 0.0016568756, -0.0008546477, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma tokenised_sentences  \\\n",
       "0        best time                  2        [best, time]   \n",
       "1       worst time                  2       [worst, time]   \n",
       "2       age wisdom                  2       [age, wisdom]   \n",
       "3      age foolish                  2      [age, foolish]   \n",
       "\n",
       "                                      doc_vector_w2v  \n",
       "0  [-0.008673585, 0.00289795, 0.0021581696, -0.00...  \n",
       "1  [-0.007879351, 0.0024533845, -0.0009934164, 0....  \n",
       "2  [-0.0043894527, 0.004767893, 0.0024528443, 0.0...  \n",
       "3  [-0.00022083164, 0.0016568756, -0.0008546477, ...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['doc_vector_w2v'] = df.tokenised_sentences.apply(lambda x : document_vector(x, model.wv))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.2\n",
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "print(gensim.__version__)\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "wv = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approax 200MB Size\n",
    "# Save Embeddings\n",
    "# wv.save('embeddings/50d_glove_vec.kv')\n",
    "\n",
    "# Load Embeddings\n",
    "# from gensim.models import KeyedVectors\n",
    "# wv = KeyedVectors.load('embeddings/50d_glove_vec.kv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
